Agenda
==========================================
Doing data processing using Spark (Tips) - read the data and transforming to a hive parquets
		ingesting json using spark-sql
	saving to hdfs


Various ways of integrating Hive and Spark (Reviews)
	saveAsTable
	insertInto
	saveAs a file and build a hive table on the output


Normalizing and denormalizing dataset into hive tables  (User)

initial import subsequent import
		When you always get a complete snapshot from source
			Without partition
				drop and reload
				insert overwrite
			With partition
				union with existing dataset, 
		When you get a delta or incremental dataset
			process normally and append to table

Various complex data structures in Hive through spark (User and Businesses)
	Nested data structures 
		denormalization

Time
	As a dimension
	For partitioning


Exporting some of the processed datasets to RDBMS (User)
	Exporting the business


Packaging as the executable using sbt and spark-submit (Tips & Reviews)



Looking forward
========================
	- Handling sparse attributes (Businesses) - Hive|HBase|Phoenix
	- Network analysis and business insights (Users, Reviews and Businesses)
